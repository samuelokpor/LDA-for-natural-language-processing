{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/spam_ham_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dirichlet, hyperparams, tokenizer\n",
    "ALPHA = 0.1\n",
    "BETA = 0.1\n",
    "NUM_TOPICS = 20\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frequencies(data, max_docs=10000):\n",
    "    freqs = Counter()\n",
    "    all_stopwords = sp.Defaults.stop_words\n",
    "    all_stopwords.add(\"enron\")\n",
    "    nr_tokens = 0\n",
    "\n",
    "    for doc in data[:max_docs]:\n",
    "        tokens = sp.tokenizer(doc)\n",
    "        for token in tokens:\n",
    "            token_text = token.text.lower()\n",
    "            if token_text not in all_stopwords and token.is_alpha:\n",
    "                nr_tokens += 1\n",
    "                freqs[token_text] += 1\n",
    "\n",
    "    return freqs\n",
    "\n",
    "def get_vocab(freqs, freq_threshold=3):\n",
    "    vocab = {}\n",
    "    vocab_idx_str = {}\n",
    "    vocab_idx = 0\n",
    "\n",
    "    for word in freqs:\n",
    "        if freqs[word] >= freq_threshold:\n",
    "            vocab[word] = vocab_idx\n",
    "            vocab_idx_str[vocab_idx] = word\n",
    "            vocab_idx += 1\n",
    "    \n",
    "    return vocab, vocab_idx_str\n",
    "\n",
    "\n",
    "def tokenize_dataset(data, vocab, max_docs=10000):\n",
    "    nr_tokens = 0\n",
    "    nr_docs = 0\n",
    "    docs = []\n",
    "\n",
    "    for doc in data[:max_docs]:\n",
    "        tokens = sp.tokenizer(doc)\n",
    "\n",
    "        if len(tokens) > 1:\n",
    "            doc = []\n",
    "            for token in tokens:\n",
    "                token_text = token.text.lower()\n",
    "                if token_text in vocab:\n",
    "                    doc.append(token_text)\n",
    "                    nr_tokens += 1\n",
    "            nr_docs += 1\n",
    "            docs.append(doc)\n",
    "\n",
    "    print(f\"Number of Labels : {nr_docs}\")\n",
    "    print(f\"Number of tokens: {nr_tokens}\")\n",
    "    \n",
    "\n",
    "    #Numericalize\n",
    "    corpus = []\n",
    "    for doc in docs:\n",
    "        corpus_d = []\n",
    "\n",
    "        for token in doc:\n",
    "            corpus_d.append(vocab[token])\n",
    "\n",
    "        corpus.append(np.array(corpus_d))\n",
    "\n",
    "    return docs, corpus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wherever', 'doing', 'nor', 'several', 'nowhere', 'up', 'give', 'made', 'last', 'me', 'they', 'in', 'seeming', 'someone', 'more', 'get', 'regarding', 'really', 'say', 'hereupon', 'unless', 'ours', 'something', 'seem', '’m', 'always', 'alone', 'can', 'go', 'serious', 'them', 'used', 'be', 'itself', 'whatever', 'sixty', \"n't\", 'he', 'upon', 'empty', 'with', 'themselves', 'throughout', 'top', 'using', 'might', 'hers', 'thru', 'five', 'been', 'during', 'indeed', 'noone', 'not', 'your', 'our', 'within', 'somewhere', 'thus', 'least', 'all', 'because', '’re', 'onto', 'thereby', 'twenty', 'else', 'has', 'meanwhile', 'four', 'latter', '‘ll', 'first', 'latterly', 'former', 'about', 'everywhere', 'though', 'hereby', 'here', 'being', 'herein', 'yours', 'n’t', 'among', 'ca', 'over', 'some', 'part', 'most', 'may', 'that', 'neither', 'yourselves', 'along', 'nobody', 'herself', '’ll', 'will', 'whose', 'must', 'somehow', 'back', '’s', 'until', 'one', 'keep', 'this', 'further', 'their', 'sometime', 'is', 'why', 'still', 'what', 'after', 'two', 'sometimes', 'thereafter', 'whereby', \"'ve\", 'well', 'too', 'except', 'thereupon', 'toward', 'call', 'who', 'have', 'ten', 'nothing', 'nine', 'whence', 'for', 'through', 'which', 'anyway', 'before', 'an', 'were', 'only', 'yourself', 'becomes', 'less', 'towards', 'under', 'between', 'where', 'anyhow', 'either', 'fifteen', 'amount', 'am', 'against', 'as', 'rather', 'very', 'become', 'i', 'eight', 'namely', 'take', 'again', 'whom', 'whether', 'whole', 'seems', 'while', 'around', 'whereupon', 'moreover', 'hence', 'same', 'afterwards', '’d', 'its', 'from', '‘ve', 'but', 'hundred', 'n‘t', '‘re', 'anywhere', 'few', 'her', 'amongst', 'so', 'elsewhere', 'across', 'front', 'if', 'or', 'everything', 'also', 'bottom', 'full', 'beforehand', 'make', 'him', 'becoming', 'down', 'had', 'below', 'to', 'much', 'cannot', 'became', 'everyone', '‘s', 'nevertheless', 'his', 'therefore', 'besides', 'enron', 'formerly', 'do', 'into', 'due', 'without', 'my', 'there', 'above', 'myself', 'such', 'next', 'anything', 'out', 'any', 'then', 'please', 'whither', 'would', 're', 'behind', 'us', 'did', 'beyond', 'hereafter', 'fifty', 'off', 'on', 'himself', 'however', 'we', 'often', 'beside', 'the', 'she', 'various', 'at', 'put', \"'m\", 'those', 'whoever', 'third', 'yet', 'already', 'was', '’ve', 'no', 'every', 'whereafter', 'side', 'even', 'are', 'perhaps', 'of', 'does', 'once', 'twelve', 'never', 'three', 'wherein', 'than', 'therein', 'whereas', 'ever', 'a', 'see', \"'ll\", 'both', 'another', 'otherwise', 'mostly', 'done', 'almost', 'move', 'other', 'via', 'seemed', '‘m', 'name', 'it', 'forty', 'own', 'since', 'anyone', 'by', 'now', 'quite', 'thence', \"'d\", 'eleven', 'together', 'show', 'enough', 'six', 'could', 'although', \"'s\", 'ourselves', 'how', 'per', 'should', 'others', \"'re\", 'when', 'just', 'you', 'many', 'none', 'mine', 'and', 'each', 'these', 'whenever', '‘d'}\n"
     ]
    }
   ],
   "source": [
    "print(sp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Labels : 52\n",
      "Number of tokens: 7092\n",
      "VOcab size : 692\n"
     ]
    }
   ],
   "source": [
    "data = df['text'].sample(frac=0.01, random_state=42).values\n",
    "freqs = generate_frequencies(data)\n",
    "vocab, vocab_idx_str = get_vocab(freqs)\n",
    "docs, corpus = tokenize_dataset(data, vocab)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"VOcab size : {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'label', 'text', 'label_num'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0bee444e958b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnkw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnkw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLDS_Collapsed_Gibbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-0bee444e958b>\u001b[0m in \u001b[0;36mLDS_Collapsed_Gibbs\u001b[1;34m(corpus, num_iter)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0mp_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mndk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mALPHA\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnkw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mBETA\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mBETA\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                 \u001b[0mtopic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp_z\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;31m#update n parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\anaconda3b\\lib\\random.py\u001b[0m in \u001b[0;36mchoices\u001b[1;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[0;32m    355\u001b[0m                 \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mcum_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_itertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot specify both weights and cumulative weights'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def LDS_Collapsed_Gibbs(corpus, num_iter=200):\n",
    "    #Initialize counts and Z\n",
    "    Z = []\n",
    "    num_docs = len(corpus)\n",
    "    for _, doc in enumerate(corpus):\n",
    "        Zd = np.random.randint(low=0, high=NUM_TOPICS, size=(len(doc)))\n",
    "        Z.append(Zd)\n",
    "\n",
    "    ndk = np.zeros((num_docs, NUM_TOPICS))\n",
    "    for d in range(num_docs):\n",
    "        for k in range(NUM_TOPICS):\n",
    "            ndk[d, k] = np.sum(Z[d] == k)\n",
    "\n",
    "    nkw = np.zeros((NUM_TOPICS, vocab_size))\n",
    "    for doc_idx, doc in enumerate(corpus):\n",
    "        for i, word in enumerate(doc):\n",
    "            topic = Z[doc_idx][i]\n",
    "            nkw[topic, word] += 1\n",
    "\n",
    "    nk = np.sum(nkw, axis=1)\n",
    "    topic_list = [i for i in range(NUM_TOPICS)]\n",
    "\n",
    "    #loop \n",
    "    for _ in tqdm(range(num_iter)):\n",
    "        for doc_idx, doc in enumerate(corpus):\n",
    "            for i in range(len(doc)):\n",
    "                word = doc[i]\n",
    "                topic = Z[doc_idx][i]\n",
    "\n",
    "                #remove z_i because conditioned on z_(-i)\n",
    "                ndk[doc_idx, topic] -= 1\n",
    "                nkw[topic, word] -= 1\n",
    "                nk[topic] -= 1\n",
    "\n",
    "                p_z = (ndk[doc_idx, :] + ALPHA) * (nkw[:, word] + BETA) / (nk[:] + BETA*vocab_size)\n",
    "                topic = random.choices(topic_list, weights=p_z, k=1)[0]\n",
    "\n",
    "                #update n parameters\n",
    "                Z[doc_idx][i] = topic\n",
    "                ndk[doc_idx, topic] += 1\n",
    "                nkw[topic, word] += 1\n",
    "                nk[topic] += 1\n",
    "    \n",
    "    return Z, ndk, nkw, nk\n",
    "\n",
    "Z, ndk, nkw, nk = LDS_Collapsed_Gibbs(corpus)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
